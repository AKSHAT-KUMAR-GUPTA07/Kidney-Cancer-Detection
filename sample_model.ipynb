{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "044b487b-e075-44be-ac81-ac7b2a872de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from PIL import Image\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('kidney_tumor_training.log', mode='w'),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ff01231-5e6a-4182-9e8f-c905cbf5973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kidney_tumor_data(excel_path, base_scan_path):\n",
    "    logging.info(f\"Starting data loading from {excel_path}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df = pd.read_excel(excel_path)\n",
    "    df['binary_label'] = df['Situation'].map({\n",
    "        'Tumor': 1, \n",
    "        'Normal case with cyst': 0, \n",
    "        'Normal case': 0\n",
    "    })\n",
    "    \n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    skipped_patients = 0\n",
    "    \n",
    "    for patient_id in df.index:\n",
    "        patient_folder = os.path.join(base_scan_path, f\"{patient_id:02d}\")\n",
    "        \n",
    "        if os.path.exists(patient_folder):\n",
    "            patient_label = df.loc[patient_id, 'binary_label']\n",
    "            \n",
    "            for subfolder in os.listdir(patient_folder):\n",
    "                subfolder_path = os.path.join(patient_folder, subfolder)\n",
    "                \n",
    "                if os.path.isdir(subfolder_path):\n",
    "                    for img_file in os.listdir(subfolder_path):\n",
    "                        if img_file.endswith('.jpg'):\n",
    "                            img_path = os.path.join(subfolder_path, img_file)\n",
    "                            image_paths.append(img_path)\n",
    "                            labels.append(patient_label)\n",
    "        else:\n",
    "            skipped_patients += 1\n",
    "    \n",
    "    logging.info(f\"Data Loading Time: {time.time() - start_time:.2f} seconds\")\n",
    "    logging.info(f\"Total Images: {len(image_paths)}\")\n",
    "    logging.info(f\"Skipped Patients: {skipped_patients}\")\n",
    "    logging.info(f\"Class Distribution:\")\n",
    "    logging.info(f\"  Negative Class: {labels.count(0)} ({labels.count(0)/len(labels)*100:.2f}%)\")\n",
    "    logging.info(f\"  Positive Class: {labels.count(1)} ({labels.count(1)/len(labels)*100:.2f}%)\")\n",
    "    \n",
    "    return image_paths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11ea1238-a71a-4e01-9dd2-72e147708313",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KidneyTumorDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.Grayscale(num_output_channels=3),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "            image = self.transform(image)\n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing image {self.image_paths[idx]}: {e}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee78c203-0620-4021-9b3d-f9d8b6a6643a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(image_paths, labels, batch_size=32):\n",
    "    logging.info(\"Creating train-validation split\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        image_paths, labels, \n",
    "        test_size=0.2, \n",
    "        stratify=labels, \n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    train_dataset = KidneyTumorDataset(X_train, y_train)\n",
    "    val_dataset = KidneyTumorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Data Loader Creation Time: {time.time() - start_time:.2f} seconds\")\n",
    "    logging.info(f\"Train Batches: {len(train_loader)}\")\n",
    "    logging.info(f\"Validation Batches: {len(val_loader)}\")\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d30a5ff6-f6a2-4ccc-99bf-1c98a0ee2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTumorClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * 32 * 32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc_layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1adb4a0-6b62-4499-93f4-763488ef1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=5):\n",
    "    logging.info(\"Starting Model Training\")\n",
    "    total_start_time = time.time()\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        \n",
    "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
    "            if images is None or labels is None:\n",
    "                continue\n",
    "            \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_train_loss += loss.item()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            total_correct += (predictions == labels).float().sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "            if batch_idx % 10 == 0:\n",
    "                logging.info(f\"Epoch {epoch+1}, Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        total_val_correct = 0\n",
    "        total_val_samples = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                if images is None or labels is None:\n",
    "                    continue\n",
    "                \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).squeeze()\n",
    "                val_loss = criterion(outputs, labels)\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                total_val_correct += (predictions == labels).float().sum().item()\n",
    "                total_val_samples += labels.size(0)\n",
    "        \n",
    "        train_accuracy = total_correct / total_samples\n",
    "        val_accuracy = total_val_correct / total_val_samples\n",
    "        \n",
    "        logging.info(f\"Epoch {epoch+1} Summary:\")\n",
    "        logging.info(f\"  Train Loss: {total_train_loss/len(train_loader):.4f}\")\n",
    "        logging.info(f\"  Train Accuracy: {train_accuracy:.4f}\")\n",
    "        logging.info(f\"  Validation Loss: {total_val_loss/len(val_loader):.4f}\")\n",
    "        logging.info(f\"  Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        logging.info(f\"  Epoch Time: {time.time() - epoch_start_time:.2f} seconds\")\n",
    "    \n",
    "    logging.info(f\"Total Training Time: {time.time() - total_start_time:.2f} seconds\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e065f418-4659-4a23-b7bd-c0cba5965ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    logging.info(\"Starting Model Evaluation\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            if images is None or labels is None:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    logging.info(\"\\nClassification Report:\\n\" + report)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    logging.info(\"\\nConfusion Matrix:\\n\" + str(cm))\n",
    "    \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    logging.info(\"Confusion matrix plot saved\")\n",
    "    \n",
    "    return report, cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09c8f28e-87ba-45c8-85d2-14da4369a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_loader):\n",
    "    logging.info(\"Starting Model Evaluation\")\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            if images is None or labels is None:\n",
    "                continue\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predictions.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    # Classification Report\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    logging.info(\"\\nClassification Report:\\n\" + report)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    logging.info(\"\\nConfusion Matrix:\\n\" + str(cm))\n",
    "    \n",
    "    return report, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93db6773-4123-4490-a2b2-7f29778a9c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-25 20:29:05,571 - INFO: Starting data loading from Dataset/00Kidney_Patients.xlsx\n",
      "2025-01-25 20:29:05,904 - INFO: Data Loading Time: 0.33 seconds\n",
      "2025-01-25 20:29:05,904 - INFO: Total Images: 7701\n",
      "2025-01-25 20:29:05,904 - INFO: Skipped Patients: 10\n",
      "2025-01-25 20:29:05,904 - INFO: Class Distribution:\n",
      "2025-01-25 20:29:05,919 - INFO:   Negative Class: 3570 (46.36%)\n",
      "2025-01-25 20:29:05,920 - INFO:   Positive Class: 4131 (53.64%)\n",
      "2025-01-25 20:29:05,922 - INFO: Creating train-validation split\n",
      "2025-01-25 20:29:05,925 - INFO: Data Loader Creation Time: 0.00 seconds\n",
      "2025-01-25 20:29:05,925 - INFO: Train Batches: 193\n",
      "2025-01-25 20:29:05,925 - INFO: Validation Batches: 49\n",
      "2025-01-25 20:29:05,958 - INFO: Starting Model Training\n",
      "2025-01-25 20:29:06,422 - INFO: Epoch 1, Batch 0: Loss = 0.7009\n",
      "2025-01-25 20:29:10,789 - INFO: Epoch 1, Batch 10: Loss = 0.6947\n",
      "2025-01-25 20:29:15,177 - INFO: Epoch 1, Batch 20: Loss = 0.6529\n",
      "2025-01-25 20:29:19,946 - INFO: Epoch 1, Batch 30: Loss = 0.6705\n",
      "2025-01-25 20:29:24,343 - INFO: Epoch 1, Batch 40: Loss = 0.6725\n",
      "2025-01-25 20:29:28,716 - INFO: Epoch 1, Batch 50: Loss = 0.5302\n",
      "2025-01-25 20:29:33,344 - INFO: Epoch 1, Batch 60: Loss = 0.6373\n",
      "2025-01-25 20:29:37,716 - INFO: Epoch 1, Batch 70: Loss = 0.4930\n",
      "2025-01-25 20:29:42,015 - INFO: Epoch 1, Batch 80: Loss = 0.4339\n",
      "2025-01-25 20:29:46,368 - INFO: Epoch 1, Batch 90: Loss = 0.5366\n",
      "2025-01-25 20:29:50,685 - INFO: Epoch 1, Batch 100: Loss = 0.3306\n",
      "2025-01-25 20:29:54,968 - INFO: Epoch 1, Batch 110: Loss = 0.2225\n",
      "2025-01-25 20:29:59,296 - INFO: Epoch 1, Batch 120: Loss = 0.2237\n",
      "2025-01-25 20:30:03,677 - INFO: Epoch 1, Batch 130: Loss = 0.2661\n",
      "2025-01-25 20:30:07,997 - INFO: Epoch 1, Batch 140: Loss = 0.3113\n",
      "2025-01-25 20:30:12,293 - INFO: Epoch 1, Batch 150: Loss = 0.2002\n",
      "2025-01-25 20:30:16,573 - INFO: Epoch 1, Batch 160: Loss = 0.1164\n",
      "2025-01-25 20:30:20,937 - INFO: Epoch 1, Batch 170: Loss = 0.1792\n",
      "2025-01-25 20:30:25,271 - INFO: Epoch 1, Batch 180: Loss = 0.1042\n",
      "2025-01-25 20:30:29,537 - INFO: Epoch 1, Batch 190: Loss = 0.1196\n",
      "2025-01-25 20:30:44,092 - INFO: Epoch 1 Summary:\n",
      "2025-01-25 20:30:44,092 - INFO:   Train Loss: 0.3878\n",
      "2025-01-25 20:30:44,092 - INFO:   Train Accuracy: 0.7990\n",
      "2025-01-25 20:30:44,092 - INFO:   Validation Loss: 0.0434\n",
      "2025-01-25 20:30:44,099 - INFO:   Validation Accuracy: 0.9864\n",
      "2025-01-25 20:30:44,100 - INFO:   Epoch Time: 98.14 seconds\n",
      "2025-01-25 20:30:44,503 - INFO: Epoch 2, Batch 0: Loss = 0.0824\n",
      "2025-01-25 20:30:48,692 - INFO: Epoch 2, Batch 10: Loss = 0.1540\n",
      "2025-01-25 20:30:52,949 - INFO: Epoch 2, Batch 20: Loss = 0.0167\n",
      "2025-01-25 20:30:57,160 - INFO: Epoch 2, Batch 30: Loss = 0.0251\n",
      "2025-01-25 20:31:01,365 - INFO: Epoch 2, Batch 40: Loss = 0.0128\n",
      "2025-01-25 20:31:05,590 - INFO: Epoch 2, Batch 50: Loss = 0.0135\n",
      "2025-01-25 20:31:09,783 - INFO: Epoch 2, Batch 60: Loss = 0.0303\n",
      "2025-01-25 20:31:14,025 - INFO: Epoch 2, Batch 70: Loss = 0.0489\n",
      "2025-01-25 20:31:18,205 - INFO: Epoch 2, Batch 80: Loss = 0.0732\n",
      "2025-01-25 20:31:22,480 - INFO: Epoch 2, Batch 90: Loss = 0.0243\n",
      "2025-01-25 20:31:26,620 - INFO: Epoch 2, Batch 100: Loss = 0.0851\n",
      "2025-01-25 20:31:30,885 - INFO: Epoch 2, Batch 110: Loss = 0.0152\n",
      "2025-01-25 20:31:35,097 - INFO: Epoch 2, Batch 120: Loss = 0.0620\n",
      "2025-01-25 20:31:39,324 - INFO: Epoch 2, Batch 130: Loss = 0.0258\n",
      "2025-01-25 20:31:43,596 - INFO: Epoch 2, Batch 140: Loss = 0.0056\n",
      "2025-01-25 20:31:47,792 - INFO: Epoch 2, Batch 150: Loss = 0.0593\n",
      "2025-01-25 20:31:51,971 - INFO: Epoch 2, Batch 160: Loss = 0.0099\n",
      "2025-01-25 20:31:56,219 - INFO: Epoch 2, Batch 170: Loss = 0.0814\n",
      "2025-01-25 20:32:00,385 - INFO: Epoch 2, Batch 180: Loss = 0.0417\n",
      "2025-01-25 20:32:04,566 - INFO: Epoch 2, Batch 190: Loss = 0.0117\n",
      "2025-01-25 20:32:18,111 - INFO: Epoch 2 Summary:\n",
      "2025-01-25 20:32:18,111 - INFO:   Train Loss: 0.0417\n",
      "2025-01-25 20:32:18,111 - INFO:   Train Accuracy: 0.9865\n",
      "2025-01-25 20:32:18,111 - INFO:   Validation Loss: 0.0089\n",
      "2025-01-25 20:32:18,119 - INFO:   Validation Accuracy: 0.9987\n",
      "2025-01-25 20:32:18,120 - INFO:   Epoch Time: 94.02 seconds\n",
      "2025-01-25 20:32:18,510 - INFO: Epoch 3, Batch 0: Loss = 0.0029\n",
      "2025-01-25 20:32:22,743 - INFO: Epoch 3, Batch 10: Loss = 0.0074\n",
      "2025-01-25 20:32:26,939 - INFO: Epoch 3, Batch 20: Loss = 0.0357\n",
      "2025-01-25 20:32:31,134 - INFO: Epoch 3, Batch 30: Loss = 0.0201\n",
      "2025-01-25 20:32:35,269 - INFO: Epoch 3, Batch 40: Loss = 0.0145\n",
      "2025-01-25 20:32:39,471 - INFO: Epoch 3, Batch 50: Loss = 0.0053\n",
      "2025-01-25 20:32:43,657 - INFO: Epoch 3, Batch 60: Loss = 0.0012\n",
      "2025-01-25 20:32:48,230 - INFO: Epoch 3, Batch 70: Loss = 0.0021\n",
      "2025-01-25 20:32:52,610 - INFO: Epoch 3, Batch 80: Loss = 0.0021\n",
      "2025-01-25 20:32:57,053 - INFO: Epoch 3, Batch 90: Loss = 0.0023\n",
      "2025-01-25 20:33:01,119 - INFO: Epoch 3, Batch 100: Loss = 0.0057\n",
      "2025-01-25 20:33:05,244 - INFO: Epoch 3, Batch 110: Loss = 0.0025\n",
      "2025-01-25 20:33:09,303 - INFO: Epoch 3, Batch 120: Loss = 0.0013\n",
      "2025-01-25 20:33:13,383 - INFO: Epoch 3, Batch 130: Loss = 0.0003\n",
      "2025-01-25 20:33:17,421 - INFO: Epoch 3, Batch 140: Loss = 0.0331\n",
      "2025-01-25 20:33:21,439 - INFO: Epoch 3, Batch 150: Loss = 0.0103\n",
      "2025-01-25 20:33:25,571 - INFO: Epoch 3, Batch 160: Loss = 0.0314\n",
      "2025-01-25 20:33:29,709 - INFO: Epoch 3, Batch 170: Loss = 0.0045\n",
      "2025-01-25 20:33:33,790 - INFO: Epoch 3, Batch 180: Loss = 0.0016\n",
      "2025-01-25 20:33:37,874 - INFO: Epoch 3, Batch 190: Loss = 0.0050\n",
      "2025-01-25 20:33:51,043 - INFO: Epoch 3 Summary:\n",
      "2025-01-25 20:33:51,043 - INFO:   Train Loss: 0.0098\n",
      "2025-01-25 20:33:51,043 - INFO:   Train Accuracy: 0.9976\n",
      "2025-01-25 20:33:51,043 - INFO:   Validation Loss: 0.0023\n",
      "2025-01-25 20:33:51,056 - INFO:   Validation Accuracy: 0.9994\n",
      "2025-01-25 20:33:51,057 - INFO:   Epoch Time: 92.94 seconds\n",
      "2025-01-25 20:33:51,436 - INFO: Epoch 4, Batch 0: Loss = 0.0027\n",
      "2025-01-25 20:33:55,453 - INFO: Epoch 4, Batch 10: Loss = 0.0103\n",
      "2025-01-25 20:33:59,483 - INFO: Epoch 4, Batch 20: Loss = 0.0019\n",
      "2025-01-25 20:34:03,569 - INFO: Epoch 4, Batch 30: Loss = 0.0024\n",
      "2025-01-25 20:34:07,613 - INFO: Epoch 4, Batch 40: Loss = 0.0007\n",
      "2025-01-25 20:34:11,637 - INFO: Epoch 4, Batch 50: Loss = 0.0004\n",
      "2025-01-25 20:34:15,692 - INFO: Epoch 4, Batch 60: Loss = 0.0005\n",
      "2025-01-25 20:34:19,739 - INFO: Epoch 4, Batch 70: Loss = 0.0042\n",
      "2025-01-25 20:34:23,786 - INFO: Epoch 4, Batch 80: Loss = 0.0001\n",
      "2025-01-25 20:34:27,989 - INFO: Epoch 4, Batch 90: Loss = 0.0075\n",
      "2025-01-25 20:34:32,051 - INFO: Epoch 4, Batch 100: Loss = 0.0015\n",
      "2025-01-25 20:34:36,105 - INFO: Epoch 4, Batch 110: Loss = 0.0004\n",
      "2025-01-25 20:34:40,609 - INFO: Epoch 4, Batch 120: Loss = 0.0036\n",
      "2025-01-25 20:34:44,918 - INFO: Epoch 4, Batch 130: Loss = 0.0004\n",
      "2025-01-25 20:34:48,918 - INFO: Epoch 4, Batch 140: Loss = 0.0004\n",
      "2025-01-25 20:34:52,902 - INFO: Epoch 4, Batch 150: Loss = 0.0017\n",
      "2025-01-25 20:34:56,911 - INFO: Epoch 4, Batch 160: Loss = 0.0002\n",
      "2025-01-25 20:35:00,913 - INFO: Epoch 4, Batch 170: Loss = 0.0013\n",
      "2025-01-25 20:35:04,926 - INFO: Epoch 4, Batch 180: Loss = 0.0032\n",
      "2025-01-25 20:35:08,926 - INFO: Epoch 4, Batch 190: Loss = 0.0185\n",
      "2025-01-25 20:35:21,957 - INFO: Epoch 4 Summary:\n",
      "2025-01-25 20:35:21,957 - INFO:   Train Loss: 0.0046\n",
      "2025-01-25 20:35:21,957 - INFO:   Train Accuracy: 0.9992\n",
      "2025-01-25 20:35:21,957 - INFO:   Validation Loss: 0.0026\n",
      "2025-01-25 20:35:21,969 - INFO:   Validation Accuracy: 0.9994\n",
      "2025-01-25 20:35:21,970 - INFO:   Epoch Time: 90.91 seconds\n",
      "2025-01-25 20:35:22,346 - INFO: Epoch 5, Batch 0: Loss = 0.0047\n",
      "2025-01-25 20:35:26,355 - INFO: Epoch 5, Batch 10: Loss = 0.0036\n",
      "2025-01-25 20:35:30,371 - INFO: Epoch 5, Batch 20: Loss = 0.0008\n",
      "2025-01-25 20:35:34,439 - INFO: Epoch 5, Batch 30: Loss = 0.0004\n",
      "2025-01-25 20:35:38,510 - INFO: Epoch 5, Batch 40: Loss = 0.0007\n",
      "2025-01-25 20:35:42,566 - INFO: Epoch 5, Batch 50: Loss = 0.0044\n",
      "2025-01-25 20:35:46,652 - INFO: Epoch 5, Batch 60: Loss = 0.0008\n",
      "2025-01-25 20:35:50,736 - INFO: Epoch 5, Batch 70: Loss = 0.0003\n",
      "2025-01-25 20:35:54,759 - INFO: Epoch 5, Batch 80: Loss = 0.0089\n",
      "2025-01-25 20:35:58,807 - INFO: Epoch 5, Batch 90: Loss = 0.0003\n",
      "2025-01-25 20:36:02,854 - INFO: Epoch 5, Batch 100: Loss = 0.0004\n",
      "2025-01-25 20:36:06,940 - INFO: Epoch 5, Batch 110: Loss = 0.0016\n",
      "2025-01-25 20:36:10,956 - INFO: Epoch 5, Batch 120: Loss = 0.0028\n",
      "2025-01-25 20:36:14,993 - INFO: Epoch 5, Batch 130: Loss = 0.0004\n",
      "2025-01-25 20:36:19,087 - INFO: Epoch 5, Batch 140: Loss = 0.0022\n",
      "2025-01-25 20:36:23,111 - INFO: Epoch 5, Batch 150: Loss = 0.0024\n",
      "2025-01-25 20:36:27,131 - INFO: Epoch 5, Batch 160: Loss = 0.0270\n",
      "2025-01-25 20:36:31,160 - INFO: Epoch 5, Batch 170: Loss = 0.0019\n",
      "2025-01-25 20:36:35,244 - INFO: Epoch 5, Batch 180: Loss = 0.0012\n",
      "2025-01-25 20:36:39,273 - INFO: Epoch 5, Batch 190: Loss = 0.0011\n",
      "2025-01-25 20:36:52,228 - INFO: Epoch 5 Summary:\n",
      "2025-01-25 20:36:52,228 - INFO:   Train Loss: 0.0032\n",
      "2025-01-25 20:36:52,228 - INFO:   Train Accuracy: 0.9994\n",
      "2025-01-25 20:36:52,228 - INFO:   Validation Loss: 0.0035\n",
      "2025-01-25 20:36:52,236 - INFO:   Validation Accuracy: 0.9994\n",
      "2025-01-25 20:36:52,236 - INFO:   Epoch Time: 90.27 seconds\n",
      "2025-01-25 20:36:52,237 - INFO: Total Training Time: 466.28 seconds\n",
      "2025-01-25 20:36:52,237 - INFO: Starting Model Evaluation\n",
      "2025-01-25 20:37:04,355 - INFO: \n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00       714\n",
      "         1.0       1.00      1.00      1.00       827\n",
      "\n",
      "    accuracy                           1.00      1541\n",
      "   macro avg       1.00      1.00      1.00      1541\n",
      "weighted avg       1.00      1.00      1.00      1541\n",
      "\n",
      "2025-01-25 20:37:04,355 - INFO: \n",
      "Confusion Matrix:\n",
      "[[713   1]\n",
      " [  0 827]]\n",
      "2025-01-25 20:37:04,355 - INFO: Saving model to kidney_tumor_model.pth\n",
      "2025-01-25 20:37:04,390 - INFO: Model saved successfully\n"
     ]
    }
   ],
   "source": [
    "def save_model(model, filepath='kidney_tumor_model.pth'):\n",
    "    logging.info(f\"Saving model to {filepath}\")\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    logging.info(\"Model saved successfully\")\n",
    "\n",
    "def main():\n",
    "    # Data preparation\n",
    "    excel_path = 'Dataset/00Kidney_Patients.xlsx'\n",
    "    base_scan_path = 'unzipped_scans'\n",
    "    \n",
    "    # Load data\n",
    "    image_paths, labels = load_kidney_tumor_data(excel_path, base_scan_path)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader, val_loader = create_data_loaders(image_paths, labels)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = SimpleTumorClassifier()\n",
    "    \n",
    "    # Train model\n",
    "    trained_model = train_model(model, train_loader, val_loader)\n",
    "    \n",
    "    # Evaluate model\n",
    "    eval_report, conf_matrix = evaluate_model(trained_model, val_loader)\n",
    "    \n",
    "    # Save model\n",
    "    save_model(trained_model)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef73b1-3975-4b32-9286-b1b3302a1deb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
